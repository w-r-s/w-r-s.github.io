<!DOCTYPE HTML>
<html lang="en">
<head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <title>Rongsheng Wang   |  ÁéãËç£ËÉú</title>
  <meta name="author" content="Yiran Geng">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" type="image/png" href="images/my/wrs_icon.jpeg">
  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?a4292ca8f2fe5fd7dc6dfd78cc894aab";
      var s = document.getElementsByTagName("script")[0]; 
      s.parentNode.insertBefore(hm, s);
    })();
  </script>
</head>
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:68%;vertical-align:middle">
              <p style="text-align:center">
                <name>Rongsheng Wang &nbsp | &nbsp ÁéãËç£ËÉú</name>
              </p>
              <!-- <p>
              I am currently working on something incredibly exciting. I received my Bachelor's degree in 2024 with highest-level honors from <a href="https://cfcs.pku.edu.cn/research/turing_program/introduction1/index.htm" target="_blank">Turing class</a> at the <a href="https://eecs.pku.edu.cn/" target="_blank">School of EECS</a>, <a href="https://english.pku.edu.cn/" target="_blank">Peking University</a> 
              , co-advised by Prof. <a href="https://zsdonghao.github.io" target="_blank">Hao Dong</a> 
              and Prof. <a href="https://www.yangyaodong.com/" target="_blank">Yaodong Yang</a>.   
              It is worth mentioning that I have a twin brother named <a class="a1" href="https://geng-haoran.github.io/" target="_blank">Haoran Geng</a>, as our experiences have shown that we are frequently mistaken for one anotherü§£.
            </p> -->
            <p>
              I am a first-year Ph.D. student at <a href="https://www.cuhk.edu.cn/en">The Chinese University of Hong Kong, Shenzhen</a>, supervised by Prof. <a href="https://wabyking.github.io/old.html" target="_blank">Benyou Wang</a>. My research focuses on exploring trustworthy medical large language models (LLMs) and multimodal large models (MLLMs), as well as exploring multimodal generative for healthcare applications.
            </p>
              <p style="text-align:center">
                <a href="mailto:rongshengwang@link.cuhk.edu.cn">Email</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=SSaBaioAAAAJ&hl=en">Google Scholar</a>
				<img src="https://img.shields.io/endpoint?url=https://w-r-s.github.io/assets/gs_data_shieldsio.json&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations" width="80"/> &nbsp/&nbsp
                <a href="https://github.com/wangrongsheng">Github</a>
				<img src="https://img.shields.io/endpoint?url=https://w-r-s.github.io/assets/stars_data_shieldsio.json&logo=github&logoColor=181717&labelColor=f6f6f6&color=9cf&style=flat&label=stars" width="40">
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/my/wrs.jpeg"><img style="width:85%;max-width:85%" alt="profile photo" src="images/my/wrs.jpeg" class="hoverZoomLink"></a>
              <!-- <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgengyiran.github.io&count_bg=%23E79717E9&title_bg=%23575454&icon=&icon_color=%23222121&title=hits&edge_flat=true"/></a> -->
              <!-- <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fgengyiran.github.io&count_bg=%23E79717E9&title_bg=%23575454&icon=&icon_color=%23222121&title=visits&edge_flat=true"/></a> -->
            </td>
          </tr>
        </tbody></table>

		<table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <br>
              <heading>News</heading>
            <br><br>
            <td style="padding:0px;width:100%;vertical-align:middle">
			  <P>
                <li>[May 2025] Two papers accepted by ACL 2025, congrats to all co-authors!
                </li>
              </P>
              <P>
                <li>[Apr. 2025] Our team won the gold medal in the AIMO-2 competition, ranking 14th out of 2213!
                </li>
              </P>
            </td>
          </tr>
        </tbody></table>
        
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <heading>Representative Publications</heading>
		  <br>
			<p>‚Ä† Equal contribution. * Corresponding author.</p>
          <br>

          <!-- RoboVerse -->
              
          <!-- <tr style="background-color: lightyellow;">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/roboverse.png' width="190"></div>
                    <br>
                  <img src='images/roboverse.png' width="190">
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/RoboVerseOrg/RoboVerse?style=flat-square&color=yellow&label=stars&logo=GitHub"> 
              </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
          </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://roboverseorg.github.io/" target="_blank">
                <papertitle>RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning
                </papertitle>
              </a>
                <br>
                <a:focus>Haoran Geng</a:focus>,
                <a:focus>Feishi Wang et al.</a:focus>
                <br>
              <a href="https://roboverseorg.github.io/static/pdfs/roboverse.pdf">Paper</a> /
              <a href="https://roboverseorg.github.io/">Project</a> / 
              <a href="https://github.com/RoboVerseOrg/RoboVerse">Code</a> /
              <a href="javascript:hideshow(document.getElementById('roboverse'))">Bibtex</a>
              <p id="roboverse" style="font:1px; display: none">
                @misc{geng2025roboverse,
                  <br>
                  title={RoboVerse: Towards a Unified Platform, Dataset and Benchmark for Scalable and Generalizable Robot Learning},
                  <br>
                  author={Haoran Geng and Feishi Wang and Songlin Wei and Yuyang Li and Bangjun Wang and Boshi An and Charlie Tianyue Cheng and Haozhe Lou and Peihao Li and Yen-Jen Wang and Yutong Liang and Dylan Goetting and Chaoyi Xu and Haozhe Chen and Yuxi Qian and Yiran Geng and Jiageng Mao and Weikang Wan and Mingtong Zhang and Jiangran Lyu and Siheng Zhao and Jiazhao Zhang and Jialiang Zhang and Chengyang Zhao and Haoran Lu and Yufei Ding and Ran Gong and Yuran Wang and Yuxuan Kuang and Ruihai Wu and Baoxiong Jia and Carlo Sferrazza and Hao Dong and Siyuan Huang and Koushil Sreenath and Yue Wang and Jitendra Malik and Pieter Abbeel}, year={2025},
                  <br>
                  primaryClass={cs.RO},
                  <br>
                  url={https://roboverse.wiki},
                  <br>
              }
              </p>
              <br>
              <em><strong>RSS 2025</strong></em>
              <p></p>
              <p>RoboVerse is a comprehensive framework for advancing robotics through a simulation platform, synthetic dataset, and unified benchmarks. </p>
            </td>
          </tr> -->
                    
          <tr style="background-color: lightyellow;">
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/paper/medgen.png' width="160"></div>
                  <br>
                  <img src='images/paper/medgen.png' width="160">
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FreedomIntelligence/MedGen?style=flat-square&color=yellow&label=stars&logo=GitHub"> 
                </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2507.05675">
                <papertitle>MedGen: Unlocking Medical Video Generation by Scaling Granularly-annotated Medical Videos</papertitle>
              </a>
              <br>
              <a:focus><strong>Rongsheng Wang‚Ä†</strong></a:focus>, 
              <a:focus>Junying Chen‚Ä†</a:focus>,
              <a:focus>Ke Ji</a:focus>,
              <a:focus>Zhenyang Cai</a:focus>,
              <a:focus>Shunian Chen</a:focus>,
              <a:focus>Benyou Wang*</a:focus>
              <br>
              <a href="https://github.com/FreedomIntelligence/MedGen">Code</a>
              /
              <a href="https://arxiv.org/abs/2507.05675">Paper</a>
              /
              <a href="https://huggingface.co/datasets/FreedomIntelligence/MedVideoCap-55K">Dataset</a>
              <br>
              <em><strong>Under Review</strong></em>
              <p></p>
              <p>
              MedGen is a medical video generation model, built on the large-scale, caption-rich MedVideoCap-55K dataset.
              </td>
          </tr>

          <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/paper/med-mat.png' width="190"></div>
                  <br>
                  <img src='images/paper/med-mat.png' width="190">
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/FreedomIntelligence/Med-MAT?style=flat-square&color=yellow&label=stars&logo=GitHub"> 
                </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://aclanthology.org/2025.acl-long.639.pdf">
                <papertitle>Exploring Compositional Generalization of Multimodal LLMs for Medical Imaging</papertitle>
              </a>
              <br>
              <a:focus>Zhenyang Cai‚Ä†</a:focus>,
              <a:focus>Junying Chen‚Ä†</a:focus>,
              <a:focus><strong>Rongsheng Wang‚Ä†</strong></a:focus>, 
              <a:focus>Weilong Wang</a:focus>,
              <a:focus>Yonglin Deng</a:focus>,
              <a:focus>Dingjie Song</a:focus>,
              <a:focus>Yize Chen</a:focus>,
              <a:focus>Zixu Zhang</a:focus>,
              <a:focus>Benyou Wang*</a:focus>
              <br>
              <a href="https://github.com/FreedomIntelligence/Med-MAT">Code</a>
              /
              <a href="https://aclanthology.org/2025.acl-long.639.pdf">Paper</a>
              /
              <a href="https://huggingface.co/datasets/FreedomIntelligence/Med-MAT">Dataset</a>
              <br>
              <span style="color: red;"><em><strong>ACL 2025 (Main)</strong></em></span>
              <p></p>
              <p>
              Multimodal LLMs achieve strong generalization in medical imaging largely through compositional generalization (CG).
              </td>
          </tr>

          <!-- <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/ManipLLM.png' width="190"></div>
                  <br>
                  <img src='images/ManipLLM.png' width="190">
                  <img alt="GitHub Repo stars" src="https://img.shields.io/github/stars/clorislili/ManipLLM?style=flat-square&color=yellow&label=stars&logo=GitHub"> 

                </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2312.16217">
                <papertitle>ManipLLM: Embodied Multimodal Large Language Model for Object-Centric Robotic Manipulation</papertitle>
              </a>
              <br>
              <a:focus>Xiaoqi Li</a:focus>, 
              <a:focus>Mingxu Zhang</a:focus>,
              <a:focus><strong>Yiran Geng</strong></a>,
              <a:focus>Haoran Geng</a:focus>,
              <a:focus>Yuxing Long</a:focus>,
              <a:focus>Yan Shen</a:focus>,
              <a:focus>Renrui Zhang</a:focus>,
              <a:focus>Jiaming Liu</a:focus>,
              <a:focus>Hao Dong</a:focus>
              <br>
              <a href="https://sites.google.com/view/manipllm">Website</a>
              /
              <a href="https://arxiv.org/abs/2312.16217">arXiv</a>
              /
              <a href="https://github.com/clorislili/ManipLLM">Code</a>
              /
              <a href="https://mp.weixin.qq.com/s/YTvN6VDmP9aSxvI5M3YVCg">ÈáèÂ≠ê‰Ωç</a>
              <br>
              <em><strong>CVPR 2024</strong></em>
              <p></p>
              <p>
              We introduce an innovative approach that leverages the robust reasoning capabilities of Multimodal Large Language Models (MLLMs).
              </td>
          </tr> -->

          <!-- <tr>
            <td style="padding:20px;width:30%;vertical-align:middle">
              <div class="one">
                <div class="two" id='malle_image'>
                  <br>
                  <img src='images/multigrasp.gif' width="190"></div>
                  <br>
                  <img src='images/multigrasp.gif' width="190">

                </div>
              <script type="text/javascript">
                function malle_start() {
                  document.getElementById('malle_image').style.opacity = "1";
                }
                function malle_stop() {
                  document.getElementById('malle_image').style.opacity = "0";
                }
                malle_stop()
              </script>
            </td>
            <td style="padding:20px;width:70%;vertical-align:middle">
              <a href="https://arxiv.org/pdf/2310.15599">
                <papertitle>Grasp Multiple Objects with One Hand</papertitle>
              </a>
              <br>
              <a:focus>Yuyang Li</a:focus>, 
              <a:focus>Bo Liu</a:focus>,
              <a:focus><strong>Yiran Geng</strong></a>,
              <a:focus>Puhao Li</a:focus>,
              <a:focus>Yaodong Yang</a:focus>,
              <a:focus>Yixin Zhu</a:focus>,
              <a:focus>Tengyu Liu</a:focus>,
              <a:focus>Siyuan Huang</a:focus>
              <br>
              <a href="https://multigrasp.github.io/">Website</a>
              /
              <a href="https://arxiv.org/pdf/2310.15599">arXiv</a>
              /
              <a href="https://github.com/MultiGrasp/MultiGrasp">Code</a>
              /
              <a href="https://github.com/MultiGrasp/MultiGrasp/tree/graspem">DataSet</a>
              <br>
              <em><strong>IROS 2024</strong> <b>Oral Presentation</b></em>
              <br>
              <em><strong>RA-L 2024</strong></em>
              <p></p>
              <p>
                We propose MultiGrasp, a two-stage framework for simultaneous multi-object grasping with multi-finger dexterous hands.              </p>
            </td>
          </tr> -->

          
    
        </tbody></table>

        <p style="text-align:center; margin-top: -20px; font-style: italic;">
          If you would like to view my other publications, you are welcome to access them on my <a href="https://scholar.google.com/citations?user=SSaBaioAAAAJ&hl=en\">Google Scholar</a>.
        </p>

        <table width="100%" align="center" border="0" cellpadding="10"><tbody>
          <br>
              <heading>Experience</heading>
          <br>
          <tr>
            <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/experience/cuhk-sz.jpg", width="100"></td>
            <td width="90%" valign="center">
              <strong>The Chinese University of Hong Kong, Shenzhen (CUHK-SZ) </strong>
              <br> 2024.09 - 2025.08
              <br> <strong>Research Assistant</strong>
              <br> Research Advisor: Prof. <a href="https://wabyking.github.io/old.html">Benyou Wang</a>
            </td>
          </tr>  
          <!-- <tr>
            <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/experience/Bigai.png", width="100"></td>
            <td width="90%" valign="center">
              <strong>Beijing Institute for General Artificial Intelligence (BIGAI) </strong>
              <br> 2022.05 - 2023.06
              <br> <strong>Research Intern</strong>
              <br> Research Advisor: Prof. <a href="https://www.yangyaodong.com/">Yaodong Yang</a> 
              <br> Academic Advisor: Prof. <a href="http://www.stat.ucla.edu/~sczhu/">Song-Chun Zhu</a>
            </td>
          </tr>
          <tr>
              <br>
              <td style="padding-left:20px;padding-right:20px;width:20%;vertical-align:middle"><img src="images/experience/PKU.png", width="100"></td>
              <td width="90%" valign="center">
                <strong>Turing Class, Peking University (PKU)</strong>
                <br> 2020.09 - 2024.06
                <br> <strong>Undergraduate Student</strong>
                <br> Research Advisor: Prof. <a href="https://zsdonghao.github.io/">Hao Dong</a>
                <br> Academic Advisor: Prof. <a href="https://zhenjiang888.github.io/">Zhenjiang Hu</a>
              </td>
          </tr> -->
        </tbody></table>

        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <br>
            <heading>Service and Teaching</heading>
            <br><br>
          <td style="padding:0px;width:100%;vertical-align:middle">
            <!-- <p>
              <li>Reviewer: The IEEE / CVF Computer Vision and Pattern Recognition Conference (CVPR 2023)</li>
            </p>
            <p>
              <li>Teaching Assistant: Probability Theory and Statistics (A) (Spring 2023)</li>
            </p>
            <p>
              <li>Teaching Assistant: Study and Practice on Topics of Frontier Computing (II) (Spring 2023) [<a href="https://github.com/GengYiran/RL-Robot-Assignment#pre-requisite-knowledge">Assignment</a>]
              </li>
            </p>
            <p>
              <li>Volunteer: Conference on Web and Internet Economics (WINE 2020)</li>
            </p> -->
            <p>
              <li>Reviewer 2025: <a href="https://www.embs.org/jbhi/">Journal of Biomedical and Health Informatics (JBHI)</a>„ÄÅ<a href="https://ieeexplore.ieee.org/xpl/RecentIssue.jsp?punumber=42">IEEE Transactions on Medical Imaging (TMI)</a></li>
            </p>
          </td>
        </tr>
        </tbody></table>

        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <br>
              <heading>Talks</heading>
            <br><br>
            <td style="padding:0px;width:100%;vertical-align:middle">
              <!-- <p>
                <li>[August 20] Student Representative Speech at the Annual School Assembly of EECS
                [<a href="pdf\Speech_2023_Assembly.pdf">Manuscript</a>]
                </li>
             </p>
              <p>
                 <li>[May 27] "RLAfford: End-to-End Affordance Learning for Robotic Manipulation" at Turing Student Forum (<strong>Outstanding Presentation Award</strong>)
                 [<a href="https://www.bilibili.com/video/BV1pM4y1i77m/?spm_id_from=333.999.0.0&vd_source=29e736b58716f318ce554795bbbe6149">Video</a>]
                 </li>
              </p>
              <p>
                <li>[May 13] "Agent Manipulation Skill Learning through Reinforcement Learning" at School of EECS Research Exhibition (<strong>Outstanding Presentation Award</strong>)
                  [<a href="pdf/eecs_poster.pdf">Poster</a>]
                </li>
              </p>
              <p>
                <li>[Mar 4] "Research Experience Sharing - PKU Undergraduates' Perspective" for Tong Class
                </li>
              </p>
              <p>
                <li>[Jan 30] "Recent Advances in Model Predictive Control" at BIGAI
                  [<a href="https://meeting.tencent.com/user-center/shared-record-info?id=e8f0edf6-b2c0-4f17-ad7d-84091ef2c327&hide_more_btn=true&from=3">Video</a>]
                </li>
              </p>
              <p>
                <li>[Dec 7 (UTC time)] Winner presentation on <a href="https://sites.google.com/view/myochallenge#h.t3275626vjox" target="_blank">NeurIPS 2022 MyoChallenge workshop</a>
                  [<a href="https://neurips.cc/virtual/2022/competition/50098">Link</a>
                  /
                  <a href="pdf/DieRotation_NIPS22.pdf">Slides</a>]
                </li>
              </p>
              <P>
                <li>[2023] "Agent Manipulation Skill Learning through Reinforcement Learning" at <a href="https://cfcs.pku.edu.cn/announcement/events/cspeertalks/index.htm" target="_blank">CS Peer Talks</a> at CFCS 
                  [<a href="https://cfcs.pku.edu.cn/announcement/events/cspeertalks/index.htm">Link</a>]
                </li>
              </P>
              <P>
                <li>[2023] "End-to-End Affordance Learning for Robotic Manipulation" at <a href="https://mp.weixin.qq.com/s?__biz=MzU0MjU5NjQ3NA==&mid=2247498147&idx=1&sn=9039ff0cf8ea6f0a6654b4ea44b586bf&chksm=fb1af5b6cc6d7ca0328ab3a7dd20dff998d3eba7d472fd1b403e8ac6dc34fdffed20c0e37fe0&mpshare=1&scene=1&srcid=1129ZzNYSx7cW5qkvJYwLwdY&sharer_sharetime=1669702563247&sharer_shareid=71465da75fb2e1df3c97cf572a6ce074#rd" target="_blank">CFCS Frontier Storm</a> at CFCS 
                  [<a href="pdf/poster.pdf">Poster</a>]
                </li>
              </P> -->
              <P>
                <li>None 
                </li>
              </P>
            </td>
          </tr>
        </tbody></table>

        <table style="width:90%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
                <br>
                <heading>Awards and Honors</heading>
                <br>
                <br>
              <td style="padding:0px;width:100%;vertical-align:middle">
                <!-- <p>
                  <li><span style="background-color: lightyellow;">2024: May Fourth Medal, Highest Honor for Peking University (<strong>Âåó‰∫¨Â§ßÂ≠¶ÊúÄÈ´òËç£Ë™â ‰∫î¬∑ÂõõÂ•ñÁ´†</strong>)üéñÔ∏è </span></li>
                </p>
                <p>
                  <li><span style="background-color: lightyellow;">2023: May Fourth Scholarship (<strong>Highest-level Scholarship</strong> for Peking University)</span></li>
                </p>
                <p>
                  <li>2023: Peking University School of EECS Outstanding Research Presentation Award</li>
                </p> -->
                <p>
                  <li>2025: Gold Medal of Kaggle AI Mathematical Olympiad - Progress Prize 2, AIMO-2 (<a href="https://www.kaggle.com/competitions/ai-mathematical-olympiad-progress-prize-2/overview">Link</a>, Rank 14/2212 globally)</li>
                </p>
                <p>
                  <li>2021: Baidu PaddlePaddle Developers Experts, PPDE (<a href="https://www.paddlepaddle.org.cn/ppdemd?n=/ppdemd/%E7%8E%8B%E8%8D%A3%E8%83%9C">Link</a>)</li>
                </p>
              </td>
            </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                    If you like it, feel free to steal this template. 
                </p>
              </td>
            </tr>
        </tbody></table>
        <script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=99b2b5&w=200&t=tt&d=w1jsZcYWX6uhqj7qrDV6Yu5EWXmHKKNJ4FzY6TnXcWs&co=f8f9fa&ct=b32b42'></script>
      </td>
    </tr>
  </table>
</body>
<!-- <footer>
<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=99b2b5&w=200&t=tt&d=w1jsZcYWX6uhqj7qrDV6Yu5EWXmHKKNJ4FzY6TnXcWs&co=f8f9fa&ct=b32b42'></script>
</footer> -->
</html>
